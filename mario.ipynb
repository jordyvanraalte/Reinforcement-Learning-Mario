{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from gym.spaces import Box\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class SkipWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.skip):\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return state, reward, done, info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GrayScaleWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        return observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ResizeWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width, height, color_depth=1):\n",
    "        super().__init__(env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.color_depth = color_depth\n",
    "        self.observation_space = Box(low=0, high=255, shape=(self.width, self.height, color_depth), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.resize(observation, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        observation = np.expand_dims(observation, -1)\n",
    "        return observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whisper/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001B[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<gym.wrappers.frame_stack.LazyFrames at 0x7fe6f87fca40>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "env = FrameStack(ResizeWrapper(GrayScaleWrapper(SkipWrapper(env, skip=4)), width=84, height=84), num_stack=4)\n",
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "((4, 84, 84, 1), 5)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    # 84x84x1\n",
    "    inputs = Input(shape=states)\n",
    "    fe1 = Conv2D(filters=32, kernel_size=8, strides=4, activation='relu')(inputs)\n",
    "    fe2 = Conv2D(filters=32, kernel_size=4, strides=2,  activation='relu')(fe1)\n",
    "    fe3 = Conv2D(filters=64, kernel_size=3, strides=1,  activation='relu')(fe2)\n",
    "    fe4 = Flatten()(fe3)\n",
    "    outputs = Dense(actions, activation=\"linear\")(fe4)\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4, 84, 84, 1)]    0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 4, 20, 20, 32)     2080      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 9, 9, 32)       16416     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 7, 7, 64)       18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 62725     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 99,717\n",
      "Trainable params: 99,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "target_model = build_model(states, actions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, model, target_model, epsilon=1.0, epsilon_min=0.1, epsilon_random_frames=50000,epsilon_greedy_frames=1000000, batch_size=32, gamma=0.99, update_target_network=10000,update_after_actions=4, max_memory_length=10000):\n",
    "        self.actions = actions\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon\n",
    "        self.epsilon_interval = epsilon - epsilon_min\n",
    "        self.epsilon_random_frames = epsilon_random_frames\n",
    "        self.epsilon_greedy_frames = epsilon_greedy_frames\n",
    "        self.memory = []\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.update_after_actions = update_after_actions\n",
    "        self.update_target_network=update_target_network\n",
    "        self.max_memory_length = max_memory_length\n",
    "        self.optimizer = Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "        self.loss_function = Huber()\n",
    "\n",
    "\n",
    "    def act(self,state, frame_count):\n",
    "        # Explore\n",
    "        if frame_count < self.epsilon_random_frames or self.epsilon > np.random.rand(1)[0]:\n",
    "            action = np.random.choice(self.actions)\n",
    "        # Exploit\n",
    "        else:\n",
    "            # convert state to a tensorflow tensor/\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # calculate new epsilon\n",
    "        self.epsilon -= self.epsilon_interval / self.epsilon_greedy_frames\n",
    "        # get the maximum between epsilon and epsilon_minimum to prevent epsilon to go below the minimum\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        return action\n",
    "\n",
    "    def cache(self, action, state, state_next, reward, done):\n",
    "        self.memory.append((action, state, state_next, reward, done))\n",
    "\n",
    "    def recall(self):\n",
    "        #print(self.batch_size)\n",
    "        # batch = np.random.sample(self.memory, self.batch_size)\n",
    "        indexes = np.random.choice(len(self.memory), self.batch_size)\n",
    "        batch = np.array([self.memory[i] for i in indexes])\n",
    "        action, state, next_state, reward, done = map(np.array, zip(*batch))\n",
    "\n",
    "        if len(self.memory) > self.max_memory_length:\n",
    "            del self.memory[:1]\n",
    "\n",
    "        return action, state, next_state, reward, done\n",
    "\n",
    "    def learn(self, frame_count):\n",
    "        action_sample, state_sample, state_next_sample, rewards_sample, done_sample = self.recall()\n",
    "\n",
    "        future_rewards = self.target_model.predict(state_next_sample)\n",
    "        done_sample = tf.convert_to_tensor([float(sample) for sample in done_sample])\n",
    "        updated_q_values = rewards_sample + self.gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1\n",
    "        )\n",
    "        # If final frame set the last value to -1\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "        # Create a mask so we only calculate loss on the updated Q-values. One hot the actions.\n",
    "        masks = tf.one_hot(action_sample, actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = model(state_sample)\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        if frame_count % self.update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            self.target_model.set_weights(model.get_weights())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "agent = Agent(actions, model, target_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-6294cd19bcac>:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch = np.array([self.memory[i] for i in indexes])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-85e8f01bcda6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mframe_count\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_after_actions\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmemory\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m             \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe_count\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mframe_count\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_target_network\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-6294cd19bcac>\u001B[0m in \u001B[0;36mlearn\u001B[0;34m(self, frame_count)\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0maction_sample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_sample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_next_sample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrewards_sample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone_sample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0mfuture_rewards\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate_next_sample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m         \u001B[0mdone_sample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msample\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdone_sample\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         updated_q_values = rewards_sample + self.gamma * tf.reduce_max(\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1978\u001B[0m       \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menumerate_epochs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Single epoch.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1979\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1980\u001B[0;31m           \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1981\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_predict_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1982\u001B[0m             \u001B[0mtmp_batch_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001B[0m in \u001B[0;36msteps\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1244\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_insufficient_data\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Set by `catch_stop_iteration`.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1245\u001B[0m         \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1246\u001B[0;31m       \u001B[0moriginal_spe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_steps_per_execution\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1247\u001B[0m       can_run_full_execution = (\n\u001B[1;32m   1248\u001B[0m           \u001B[0moriginal_spe\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001B[0m in \u001B[0;36mnumpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    673\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 674\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    675\u001B[0m     raise NotImplementedError(\n\u001B[1;32m    676\u001B[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001B[0m in \u001B[0;36mread_value\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    747\u001B[0m     \"\"\"\n\u001B[1;32m    748\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Read\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 749\u001B[0;31m       \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read_variable_op\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    750\u001B[0m     \u001B[0;31m# Return an identity so it can get placed on whatever device the context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    751\u001B[0m     \u001B[0;31m# specifies instead of the device where the variable is.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001B[0m in \u001B[0;36m_read_variable_op\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    713\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    714\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_read_variable_op\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 715\u001B[0;31m     \u001B[0mvariable_accessed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    716\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    717\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mread_and_set_handle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001B[0m in \u001B[0;36mvariable_accessed\u001B[0;34m(variable)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mvariable_accessed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvariable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m   \u001B[0;34m\"\"\"Records that `variable` was accessed for the tape and FuncGraph.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m   \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"watch_variable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    327\u001B[0m     \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwatch_variable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvariable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mvariable\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mget_default_graph\u001B[0;34m()\u001B[0m\n\u001B[1;32m   6308\u001B[0m     \u001B[0mThe\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mGraph\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mbeing\u001B[0m \u001B[0mused\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mcurrent\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6309\u001B[0m   \"\"\"\n\u001B[0;32m-> 6310\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0m_default_graph_stack\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "while episode_count < 5000:\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        env.render()\n",
    "\n",
    "        action = agent.act(state, frame_count)\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        agent.cache(action, state, state_next, reward, done)\n",
    "\n",
    "        episode_reward += reward\n",
    "        state = state_next\n",
    "\n",
    "        if frame_count % agent.update_after_actions == 0 and len(agent.memory) > agent.batch_size:\n",
    "            agent.learn(frame_count)\n",
    "\n",
    "        if frame_count % agent.update_target_network == 0:\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon: {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, agent.epsilon))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    episode_count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state, next_state, action, reward, done = map(list, zip(*agent.memory))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_history = {\n",
    "    \"episode_rewards\": episode_reward_history\n",
    "}\n",
    "pd.DataFrame(full_history).plot(figsize=(12, 8))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}